{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LABORATORIUM10_09_12_20r.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DWQt3fnoTbsR"},"source":["#CEL LABORATORIUM: \n","Na tym laboratorium mieliśmy dowiedzieć się jak zająć się typami danych często występującymi w rzeczywistych zestawach danych (brakujące wartości, zmienne kategorialne),projektować potoki w celu poprawy jakości kodu uczenia maszynowego,stosować zaawansowane techniki walidacji modeli (walidacja krzyżowa),budować najnowocześniejsze modele, które są powszechnie używane do wygrywania zawodów Kaggle (XGBoost), oraz uniknąć typowych i ważnych błędów w nauce o danych (wycieku).\n","\n","\n","Istnieje wiele sposobów, jakie dane mogą kończyć się brakującymi wartościami. Musisz więc wybrać jedną z poniższych opcji.\n","\n","Wyróżniamy :\n","#1) Prosta opcja: upuszczenie kolumny z brakami wartości\n","#2) Lepsza opcja: przypisanie\n","#3) Przedłużenie przypisania\n"]},{"cell_type":"markdown","metadata":{"id":"5E8RuXAYW9sU"},"source":["# 2. Brakujące dane"]},{"cell_type":"code","metadata":{"id":"jgAcZADrUvq9"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Ładowanie danych\n","data = pd.read_csv('/content/melb_data.csv')\n","\n","# Wybór celu\n","y = data.Price\n","\n","# Aby uprościć sprawę, użyjemy tylko predyktorów numerycznych\n","melb_predictors = data.drop(['Price'], axis=1)\n","X = melb_predictors.select_dtypes(exclude=['object'])\n","# Podziel dane na podzbiory treningowe i walidacyjne\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n","                                                      random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVk_LngiVlYG"},"source":["Zdefiniuj funkcję pomiaru jakości każdego podejścia"]},{"cell_type":"code","metadata":{"id":"AIF8MwVsUuWz"},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","# Funkcja do porównywania różnych podejść\n","def score_dataset(X_train, X_valid, y_train, y_valid):\n","    model = RandomForestRegressor(n_estimators=10, random_state=0)\n","    model.fit(X_train, y_train)\n","    preds = model.predict(X_valid)\n","    return mean_absolute_error(y_valid, preds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rRo6c-8YVslt"},"source":["1)Upuszczanie kolumn z brakami wartości"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbhUoCeKUq4M","outputId":"336d5ef4-0711-40c0-d61d-d983e76c40b6"},"source":["# Pobierz nazwy kolumn z brakami danych\n","cols_with_missing = [col for col in X_train.columns\n","                     if X_train[col].isnull().any()]\n","# Upuść kolumny w danych treningowych i walidacyjnych\n","reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n","reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n","\n","print(\"MAE z Podejścia 1 (Usuń kolumny z brakującymi wartościami):\")\n","print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE z Podejścia 1 (Usuń kolumny z brakującymi wartościami):\n","183550.22137772635\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1ORivYKyVyNe"},"source":["2) Przypisanie\n","Następnie używamy SimpleImputer, aby zastąpić brakujące wartości średnią wartością w każdej kolumnie."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BOhiZOGfWBgK","outputId":"ebd9aa33-45aa-4516-fafe-0bf57298c8a4"},"source":["from sklearn.impute import SimpleImputer\n","\n","# Imputation\n","my_imputer = SimpleImputer()\n","imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n","imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n","\n","# Imputation removed column names; put them back\n","imputed_X_train.columns = X_train.columns\n","imputed_X_valid.columns = X_valid.columns\n","\n","print(\"MAE z Podejścia 2 (Imputacja):\")\n","print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE z Podejścia 2 (Imputacja):\n","178166.46269899711\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XQXx11EpWFgk"},"source":["3) Przedłużenie przypisania\n","Następnie imputujemy brakujące wartości, jednocześnie śledząc, które wartości zostały imputowane."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37G-KLVXWOUD","outputId":"97924186-ca02-4caa-8df4-f53535e84e24"},"source":["# Utwórz kopię, aby uniknąć zmiany oryginalnych danych (podczas imputowania)\n","X_train_plus = X_train.copy()\n","X_valid_plus = X_valid.copy()\n","\n","# Utwórz nowe kolumny wskazujące, co zostanie przypisane\n","for col in cols_with_missing:\n","    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n","    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n","\n","# Imputacja\n","my_imputer = SimpleImputer()\n","imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n","imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n","\n","# Imputacja usuniętych nazw kolumn; odłóż je z powrotem\n","imputed_X_train_plus.columns = X_train_plus.columns\n","imputed_X_valid_plus.columns = X_valid_plus.columns\n","\n","print(\"MAE z podejścia 3 (rozszerzenie do przypisania):\")\n","print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE z podejścia 3 (rozszerzenie do przypisania):\n","178927.503183954\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pYNUDGLCWgqX"},"source":["Imputacja działała lepiej niż porzucanie kolumn ponieważ  \n","Dane szkoleniowe mają 10864 wierszy i 12 kolumn, z których trzy kolumny zawierają brakujące dane. W każdej kolumnie brakuje mniej niż połowy wpisów. W związku z tym upuszczenie kolumn powoduje usunięcie wielu przydatnych informacji, dlatego sensowne jest, aby imputacja działała lepiej."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ll4PsOHTWkbe","outputId":"d64f0c82-4721-4b0f-9743-c4a6982c9b03"},"source":["# Kształt danych szkoleniowych (num_rows, num_columns)\n","print(X_train.shape)\n","\n","# Liczba brakujących wartości w każdej kolumnie danych szkoleniowych\n","missing_val_count_by_column = (X_train.isnull().sum())\n","print(missing_val_count_by_column[missing_val_count_by_column > 0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(10864, 12)\n","Car               49\n","BuildingArea    5156\n","YearBuilt       4307\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"98S5NdyfXBUV"},"source":["# 3. Zmienna kategorialna"]},{"cell_type":"markdown","metadata":{"id":"T0jO-YFfXISV"},"source":["Zmienna kategorialna przyjmuje tylko ograniczoną liczbę wartości.\n","Rozważę ankietę, która pyta, jak często pomagasz kolegom w nauce i oferuje cztery opcje: „Nigdy”, „Rzadko”, „Większość dni” lub „Codziennie”.\n","Błąd pojawi się w momencie gdy podłączysz zmienne do modeli uczenia maszynowego bez ich wcześniejszego przetworzenia.\n","\n","aby uniknąć błędów mamy 3 podejścia:\n","1) Usuń zmienne kategorialne-najłatwiejszym sposobem radzenia sobie ze zmiennymi kategorialnymi jest po prostu usunięcie ich ze zbioru danych. Takie podejście będzie działać dobrze tylko wtedy, gdy kolumny nie zawierały przydatnych informacji.\n","2) Kodowanie etykiet-przypisuje każdą unikalną wartość do innej liczby całkowitej.\n","3) Kodowanie na gorąco - tworzy nowe kolumny wskazujące na obecność (lub brak) każdej możliwej wartości w oryginalnych danych.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"P9Dokh3OX45_"},"source":["Przykład:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YkgZO7_WYZp1","outputId":"104ee3e0-09fc-44e9-ef3a-92c1921b597f"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Odczyt danych\n","data = pd.read_csv('/content/melb_data.csv')\n","\n","# Oddziel cel od predyktorów\n","y = data.Price\n","X = data.drop(['Price'], axis=1)\n","\n","# Podziel dane na podzbiory treningowe i walidacyjne\n","X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n","                                                                random_state=0)\n","\n","# Usuń kolumny z brakującymi wartościami (najprostsze podejście)\n","\n","\n","cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n","X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n","X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n","\n","# „Liczność” oznacza liczbę unikatowych wartości w kolumnie\n","# Wybierz kolumny kategorialne o stosunkowo niskiej liczności (wygodne, ale dowolne)\n","low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n","                        X_train_full[cname].dtype == \"object\"]\n","\n","# Wybierz kolumny numeryczne\n","numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n","\n","# Zachowaj tylko wybrane kolumny\n","my_cols = low_cardinality_cols + numerical_cols\n","X_train = X_train_full[my_cols].copy()\n","X_valid = X_valid_full[my_cols].copy()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"gBT-dKYMYg7T"},"source":["Rzucamy okiem na dane treningowe za pomocą metody head () poniżej."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"e6Ngl6nyYdoy","outputId":"ed6f857f-7caa-4826-b44c-5b45bf932965"},"source":["X_train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Type</th>\n","      <th>Method</th>\n","      <th>Regionname</th>\n","      <th>Rooms</th>\n","      <th>Distance</th>\n","      <th>Postcode</th>\n","      <th>Bedroom2</th>\n","      <th>Bathroom</th>\n","      <th>Landsize</th>\n","      <th>Lattitude</th>\n","      <th>Longtitude</th>\n","      <th>Propertycount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12167</th>\n","      <td>u</td>\n","      <td>S</td>\n","      <td>Southern Metropolitan</td>\n","      <td>1</td>\n","      <td>5.0</td>\n","      <td>3182.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>-37.85984</td>\n","      <td>144.9867</td>\n","      <td>13240.0</td>\n","    </tr>\n","    <tr>\n","      <th>6524</th>\n","      <td>h</td>\n","      <td>SA</td>\n","      <td>Western Metropolitan</td>\n","      <td>2</td>\n","      <td>8.0</td>\n","      <td>3016.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>193.0</td>\n","      <td>-37.85800</td>\n","      <td>144.9005</td>\n","      <td>6380.0</td>\n","    </tr>\n","    <tr>\n","      <th>8413</th>\n","      <td>h</td>\n","      <td>S</td>\n","      <td>Western Metropolitan</td>\n","      <td>3</td>\n","      <td>12.6</td>\n","      <td>3020.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>555.0</td>\n","      <td>-37.79880</td>\n","      <td>144.8220</td>\n","      <td>3755.0</td>\n","    </tr>\n","    <tr>\n","      <th>2919</th>\n","      <td>u</td>\n","      <td>SP</td>\n","      <td>Northern Metropolitan</td>\n","      <td>3</td>\n","      <td>13.0</td>\n","      <td>3046.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>265.0</td>\n","      <td>-37.70830</td>\n","      <td>144.9158</td>\n","      <td>8870.0</td>\n","    </tr>\n","    <tr>\n","      <th>6043</th>\n","      <td>h</td>\n","      <td>S</td>\n","      <td>Western Metropolitan</td>\n","      <td>3</td>\n","      <td>13.3</td>\n","      <td>3020.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>673.0</td>\n","      <td>-37.76230</td>\n","      <td>144.8272</td>\n","      <td>4217.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Type Method             Regionname  ...  Lattitude  Longtitude  Propertycount\n","12167    u      S  Southern Metropolitan  ...  -37.85984    144.9867        13240.0\n","6524     h     SA   Western Metropolitan  ...  -37.85800    144.9005         6380.0\n","8413     h      S   Western Metropolitan  ...  -37.79880    144.8220         3755.0\n","2919     u     SP  Northern Metropolitan  ...  -37.70830    144.9158         8870.0\n","6043     h      S   Western Metropolitan  ...  -37.76230    144.8272         4217.0\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"zQecE0AXYk3p"},"source":["Następnie otrzymujemy listę wszystkich zmiennych kategorialnych w danych uczących.\n","\n","Robimy to, sprawdzając typ danych (lub typ) każdej kolumny. Obiekt dtype wskazuje, że kolumna zawiera tekst (są inne rzeczy, które teoretycznie mogłyby to być, ale to nie jest ważne dla naszych celów). W przypadku tego zbioru danych kolumny z tekstem wskazują zmienne kategorialne."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3p_vYJkYmvy","outputId":"e359f4f4-5276-4298-d0a1-1b5df5bfff2d"},"source":["# Pobierz listę zmiennych kategorialnych\n","s = (X_train.dtypes == 'object')\n","object_cols = list(s[s].index)\n","\n","print(\"Zmienne kategorialne:\")\n","print(object_cols)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Zmienne kategorialne:\n","['Type', 'Method', 'Regionname']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dee4KSpsY9qf"},"source":["Zdefiniowanie funkcji pomiaru jakości każdego podejścia\n","\n","Definiujemy funkcję score_dataset (), aby porównać trzy różne podejścia do pracy ze zmiennymi kategorialnymi. Ta funkcja raportuje średni błąd bezwzględny (MAE) z losowego modelu lasu. Ogólnie rzecz biorąc, chcemy, aby MAE było jak najniższe!"]},{"cell_type":"code","metadata":{"id":"MK5MUOEKY2p0"},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","# Funkcja do porównywania różnych podejść\n","def score_dataset(X_train, X_valid, y_train, y_valid):\n","    model = RandomForestRegressor(n_estimators=100, random_state=0)\n","    model.fit(X_train, y_train)\n","    preds = model.predict(X_valid)\n","    return mean_absolute_error(y_valid, preds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMuJxyYKZDbK"},"source":["Wynik z podejścia 1 (porzuć zmienne kategorialne)\n","\n","Porzucamy kolumny obiektów metodą select_dtypes ().\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JfhrnGJbYxFI","outputId":"320cacb4-7e2d-4d91-c1b2-eb6b7e35ff0c"},"source":["drop_X_train = X_train.select_dtypes(exclude=['object'])\n","drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n","\n","print(\"MAE z Podejścia 1 (Usuń zmienne kategorialne):\")\n","print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE z Podejścia 1 (Usuń zmienne kategorialne):\n","175703.48185157913\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2sr-mZxHZKA1"},"source":["Wynik z podejścia 2 (kodowanie etykiet)\n","\n","Scikit-learn ma klasę LabelEncoder, której można użyć do uzyskania kodowania etykiet. W pętli przeglądamy zmienne kategorialne i stosujemy koder etykiet oddzielnie do każdej kolumny."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFDKuLXUZKsS","outputId":"50fe3aa3-ccad-4a7c-c0e8-f4e04930627b"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Kopia, aby uniknąć zmiany oryginalnych danych\n","label_X_train = X_train.copy()\n","label_X_valid = X_valid.copy()\n","\n","# Zastosowanie kodera etykiet do każdej kolumny z danymi kategorycznymi\n","label_encoder = LabelEncoder()\n","for col in object_cols:\n","    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n","    label_X_valid[col] = label_encoder.transform(X_valid[col])\n","\n","print(\"MAE z podejścia 2 (kodowanie etykiet):\") \n","print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE z podejścia 2 (kodowanie etykiet):\n","165936.40548390493\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5SKM0VUAZS-V"},"source":["Wynik z podejścia 3 (One-Hot Encoding)\n","\n","Używamy klasy OneHotEncoder ze scikit-learn, aby uzyskać kodowanie na gorąco. Istnieje wiele parametrów, za pomocą których można dostosować jego zachowanie.\n","Ustawiliśmy handle_unknown = 'ignore', aby uniknąć błędów, gdy dane walidacyjne zawierają klasy, które nie są reprezentowane w danych szkoleniowych, oraz\n","ustawienie sparse = False zapewnia, że ​​zakodowane kolumny są zwracane jako tablica numpy (zamiast rzadkiej macierzy).\n","\n","Aby użyć kodera, dostarczamy tylko kolumny kategorialne, które chcemy zakodować na gorąco. Na przykład, aby zakodować dane szkoleniowe, dostarczamy X_train[object_cols].(obiekt_cols w komórce kodu poniżej to lista nazw kolumn z danymi kategorialnymi, dlatego X_train [object_cols] zawiera wszystkie dane kategoryczne w zestawie uczącym)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dy3GOWSdZggi","outputId":"21989790-1477-4643-8122-2aa5325d2513"},"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","# Zastosowanie jednego gorącego kodera do każdej kolumny z danymi kategorycznymi\n","OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n","OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n","\n","# Usunięty indeks jednego gorącego kodowania; odłóż to z powrotem\n","OH_cols_train.index = X_train.index\n","OH_cols_valid.index = X_valid.index\n","\n","# Usuwanie kolumny kategorycznej (zastąpione jednym gorącym kodowaniem)\n","num_X_train = X_train.drop(object_cols, axis=1)\n","num_X_valid = X_valid.drop(object_cols, axis=1)\n","\n","# Dodawanie gorących zakodowanych kolumn do funkcji numerycznych\n","OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n","OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n","\n","print(\"MAE z Approach 3 (One-Hot Encoding):\") \n","print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE z Approach 3 (One-Hot Encoding):\n","166089.4893009678\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KkKH9nBFaIvA"},"source":["Które podejście jest najlepsze?\n","\n","W tym przypadku upuszczenie kolumn kategorycznych (Podejście 1) wypadło najgorzej, ponieważ miało najwyższy wynik MAE. Jeśli chodzi o pozostałe dwa podejścia, ponieważ zwrócone wyniki MAE mają tak bliską wartość, wydaje się, że nie ma żadnych znaczących korzyści dla jednego nad drugim.\n","\n","Ogólnie rzecz biorąc, kodowanie typu one-hot (podejście 3) zazwyczaj działa najlepiej, a usunięcie kolumn kategorialnych (podejście 1) zazwyczaj działa najgorzej, ale różni się w zależności od przypadku."]},{"cell_type":"markdown","metadata":{"id":"olMdYRHZaWll"},"source":["# 4.Potoki\n","\n","Wprowadzenie\n","\n","Potoki to prosty sposób na zorganizowanie wstępnego przetwarzania danych i modelowania kodu. W szczególności potok obejmuje etapy wstępnego przetwarzania i modelowania, dzięki czemu można używać całego pakietu tak, jakby był to pojedynczy krok.\n","\n","Wielu analityków danych hakuje razem modele bez rurociągów, ale rurociągi mają kilka ważnych zalet. Należą do nich:\n","\n","Czystszy kod: rozliczanie danych na każdym etapie przetwarzania wstępnego może być skomplikowane. Dzięki potokowi nie musisz ręcznie śledzić danych treningowych i walidacyjnych na każdym etapie.\n","Mniej błędów: istnieje mniej okazji do błędnego zastosowania kroku lub zapomnienia o kroku przetwarzania wstępnego.\n","Łatwiejsze w produkcji: Przeniesienie modelu z prototypu na coś, co można wdrożyć na dużą skalę, może być zaskakująco trudne. Nie będziemy tu wchodzić w wiele powiązanych problemów, ale rurociągi mogą pomóc.\n","Więcej opcji sprawdzania poprawności modelu: W następnym samouczku zobaczysz przykład, który obejmuje weryfikację krzyżową.\n"]},{"cell_type":"code","metadata":{"id":"50g0Q7oHaql7"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Odczyt danych\n","data = pd.read_csv('/content/melb_data.csv')\n","\n","# Oddziel cel od predyktorów\n","y = data.Price\n","X = data.drop(['Price'], axis=1)\n","\n","# Podziel dane na podzbiory treningowe i walidacyjne\n","X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n","                                                                random_state=0)\n","\n","# „Liczność” oznacza liczbę unikatowych wartości w kolumnie\n","# Wybierz kolumny kategorialne o stosunkowo niskiej liczności (wygodne, ale dowolne)\n","categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n","                        X_train_full[cname].dtype == \"object\"]\n","\n","\n","\n","# Wybierz kolumny numeryczne\n","numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n","\n","# Zachowaj tylko wybrane kolumny\n","my_cols = categorical_cols + numerical_cols\n","X_train = X_train_full[my_cols].copy()\n","X_valid = X_valid_full[my_cols].copy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"b6glKt1ibCal","outputId":"1dc6e682-c2ab-4352-bbff-871aa299b642"},"source":["X_train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Type</th>\n","      <th>Method</th>\n","      <th>Regionname</th>\n","      <th>Rooms</th>\n","      <th>Distance</th>\n","      <th>Postcode</th>\n","      <th>Bedroom2</th>\n","      <th>Bathroom</th>\n","      <th>Car</th>\n","      <th>Landsize</th>\n","      <th>BuildingArea</th>\n","      <th>YearBuilt</th>\n","      <th>Lattitude</th>\n","      <th>Longtitude</th>\n","      <th>Propertycount</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12167</th>\n","      <td>u</td>\n","      <td>S</td>\n","      <td>Southern Metropolitan</td>\n","      <td>1</td>\n","      <td>5.0</td>\n","      <td>3182.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>1940.0</td>\n","      <td>-37.85984</td>\n","      <td>144.9867</td>\n","      <td>13240.0</td>\n","    </tr>\n","    <tr>\n","      <th>6524</th>\n","      <td>h</td>\n","      <td>SA</td>\n","      <td>Western Metropolitan</td>\n","      <td>2</td>\n","      <td>8.0</td>\n","      <td>3016.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>193.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>-37.85800</td>\n","      <td>144.9005</td>\n","      <td>6380.0</td>\n","    </tr>\n","    <tr>\n","      <th>8413</th>\n","      <td>h</td>\n","      <td>S</td>\n","      <td>Western Metropolitan</td>\n","      <td>3</td>\n","      <td>12.6</td>\n","      <td>3020.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>555.0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>-37.79880</td>\n","      <td>144.8220</td>\n","      <td>3755.0</td>\n","    </tr>\n","    <tr>\n","      <th>2919</th>\n","      <td>u</td>\n","      <td>SP</td>\n","      <td>Northern Metropolitan</td>\n","      <td>3</td>\n","      <td>13.0</td>\n","      <td>3046.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>265.0</td>\n","      <td>NaN</td>\n","      <td>1995.0</td>\n","      <td>-37.70830</td>\n","      <td>144.9158</td>\n","      <td>8870.0</td>\n","    </tr>\n","    <tr>\n","      <th>6043</th>\n","      <td>h</td>\n","      <td>S</td>\n","      <td>Western Metropolitan</td>\n","      <td>3</td>\n","      <td>13.3</td>\n","      <td>3020.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>673.0</td>\n","      <td>673.0</td>\n","      <td>1970.0</td>\n","      <td>-37.76230</td>\n","      <td>144.8272</td>\n","      <td>4217.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Type Method             Regionname  ...  Lattitude  Longtitude  Propertycount\n","12167    u      S  Southern Metropolitan  ...  -37.85984    144.9867        13240.0\n","6524     h     SA   Western Metropolitan  ...  -37.85800    144.9005         6380.0\n","8413     h      S   Western Metropolitan  ...  -37.79880    144.8220         3755.0\n","2919     u     SP  Northern Metropolitan  ...  -37.70830    144.9158         8870.0\n","6043     h      S   Western Metropolitan  ...  -37.76230    144.8272         4217.0\n","\n","[5 rows x 15 columns]"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"markdown","metadata":{"id":"mbwHepYibHBi"},"source":["Krok 1: Zdefiniuj kroki przetwarzania wstępneg\n","\n","Podobnie do tego, jak potok łączy etapy przetwarzania wstępnego i modelowania, używamy klasy ColumnTransformer do łączenia różnych etapów przetwarzania wstępnego. Poniższy kod:umieszcza brakujące wartości w danych liczbowych,przypisuje brakujące wartości i stosuje kodowanie typu one-hot do danych kategorialnych.\n","\n"]},{"cell_type":"code","metadata":{"id":"hOA71fzqbPiH"},"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Wstępne przetwarzanie danych liczbowych\n","\n","\n","numerical_transformer = SimpleImputer(strategy='constant')\n","\n","# Przetwarzanie wstępne dla danych kategorycznych\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","# Wstępne przetwarzanie pakietów dla danych liczbowych i kategorialnych\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numerical_transformer, numerical_cols),\n","        ('cat', categorical_transformer, categorical_cols)\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JyTQ_NRbY5m"},"source":["Krok 2: Definiowanie modelu.\n","\n","Następnie definiujemy losowy model lasu za pomocą znanej klasy RandomForestRegressor."]},{"cell_type":"code","metadata":{"id":"o2xRzvi3baF5"},"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","model = RandomForestRegressor(n_estimators=100, random_state=0)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1i3lBFCibi3d"},"source":["Krok 3: Utworzenie i ocena potok u\n","\n","Na koniec używamy klasy Pipeline do definiowania potoku obejmującego etapy przetwarzania wstępnego i modelowania. Należy zwrócić uwagę na kilka ważnych rzeczy:\n","Za pomocą potoku wstępnie przetwarzamy dane szkoleniowe i dopasowujemy model w jednym wierszu kodu. (W przeciwieństwie do tego, bez potoku, musimy wykonywać imputację, kodowanie typu one-hot i trenowanie modelu w oddzielnych krokach. Staje się to szczególnie kłopotliwe, jeśli mamy do czynienia zarówno ze zmiennymi numerycznymi, jak i kategorialnymi!)\n","Wraz z potokiem dostarczamy nieprzetworzone funkcje w X_valid do polecenia Predict (), a potok automatycznie przetwarza funkcje przed wygenerowaniem prognoz. (Jednak bez potoku musimy pamiętać o wstępnym przetworzeniu danych walidacyjnych przed wykonaniem prognoz)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lbdi8E_qbpS2","outputId":"9f6ab293-bc9c-46a8-b271-862fbc3fa68a"},"source":["from sklearn.metrics import mean_absolute_error\n","\n","# Przetwarzanie wstępne i modelowanie kodu w potoku\n","my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n","                              ('model', model)\n","                             ])\n","\n","# Wstępne przetwarzanie danych treningowych, dopasowanie modelu\n","my_pipeline.fit(X_train, y_train)\n","\n","# Wstępne przetwarzanie danych walidacyjnych, uzyskiwanie prognoz\n","preds = my_pipeline.predict(X_valid)\n","\n","# Ocena modelu\n","score = mean_absolute_error(y_valid, preds)\n","print('MAE:', score)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE: 160679.18917034855\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HhDzaBYzb2hE"},"source":["Potoki są cenne do czyszczenia kodu uczenia maszynowego i unikania błędów oraz są szczególnie przydatne w przepływach pracy z zaawansowanym przetwarzaniem wstępnym danych."]},{"cell_type":"markdown","metadata":{"id":"B44c44iOcIKo"},"source":["# 5. Walidacja krzyżowa\n","Co to jest walidacja krzyżowa?\n","\n","W ramach walidacji krzyżowej uruchamiamy proces modelowania na różnych podzbiorach danych, aby uzyskać wiele miar jakości modelu.\n","Będziemy pracować z tymi samymi danymi, co w poprzednim samouczku. Ładujemy dane wejściowe w X, a dane wyjściowe w Y."]},{"cell_type":"code","metadata":{"id":"4CSV8WRpcXrS"},"source":["import pandas as pd\n","\n","# Odczyt danych\n","data = pd.read_csv('/content/melb_data.csv')\n","\n","# Wybierz podzbiór predyktorów\n","\n","\n","cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n","X = data[cols_to_use]\n","\n","# Wybór celu\n","y = data.Price\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7KwB8ysEckUw"},"source":["Następnie definiujemy potok, który używa imputera do uzupełniania brakujących wartości i losowego modelu lasu do tworzenia prognoz.\n","\n","Chociaż można przeprowadzić weryfikację krzyżową bez potoków, jest to dość trudne! Użycie potoku sprawi, że kod będzie niezwykle prosty."]},{"cell_type":"code","metadata":{"id":"9Kofuuz8co9G"},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","\n","my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n","                              ('model', RandomForestRegressor(n_estimators=50,\n","                                                              random_state=0))\n","                             ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gE8bLH-LctNN"},"source":["Otrzymujemy wyniki walidacji krzyżowej za pomocą funkcji cross_val_score () z scikit-learn. Liczbę fałd ustalamy parametrem cv."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYyt4g86cuO5","outputId":"eb923c8f-2e18-40c3-f09d-969214755302"},"source":["from sklearn.model_selection import cross_val_score\n","\n","# Pomnóż przez -1, ponieważ sklearn oblicza * ujemne * MAE\n","scores = -1 * cross_val_score(my_pipeline, X, y,\n","                              cv=5,\n","                              scoring='neg_mean_absolute_error')\n","\n","print(\"MAE scores:\\n\", scores)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE scores:\n"," [301628.7893587  303164.4782723  287298.331666   236061.84754543\n"," 260383.45111427]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xwOVZotcc1eh"},"source":["Parametr punktacji wybiera miarę jakości modelu do raportu: w tym przypadku wybraliśmy ujemny średni błąd bezwzględny (MAE). Dokumentacja scikit-learn pokazuje listę opcji.\n","\n","Zaskakujące jest to, że podajemy ujemne MAE. Scikit-learn ma konwencję, w której wszystkie metryki są zdefiniowane, więc wyższa liczba jest lepsza. Używanie tutaj negatywów pozwala im być zgodnymi z tą konwencją, chociaż negatywne MAE jest prawie niespotykane gdzie indziej.\n","\n","Zwykle potrzebujemy jednej miary jakości modelu, aby porównać alternatywne modele. Więc bierzemy średnią z eksperymentów."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFAptsBlc2Ie","outputId":"3fca08ae-8e85-4962-af8e-4a8c088faf60"},"source":["print(\"Średni wynik MAE (we wszystkich eksperymentach):\")\n","print(scores.mean())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Średni wynik MAE (we wszystkich eksperymentach):\n","277707.3795913405\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"993poUfedBmU"},"source":["Korzystanie z walidacji krzyżowej zapewnia znacznie lepszą miarę jakości modelu, z dodatkową zaletą czyszczenia naszego kodu: Trzeba pamiętać, że nie musimy już śledzić oddzielnych zestawów uczących i walidacyjnych. Tak więc, szczególnie w przypadku małych zbiorów danych, jest to dobra poprawa!"]},{"cell_type":"markdown","metadata":{"id":"nk9wLE_NdLSK"},"source":["# 6. XGBoost\n","Przez większość tego kursu dokonywałem prognoz metodą losowego lasu, która osiąga lepszą wydajność niż pojedyncze drzewo decyzyjne, po prostu poprzez uśrednienie prognoz wielu drzew decyzyjnych.\n","\n","Metodę losowego lasu nazywamy „metodą zespołową”. Z definicji metody zespołowe łączą przewidywania kilku modeli (np. Kilku drzew w przypadku lasów losowych).\n","\n","Następnie przedstawiona zostanie inna metoda zespolona zwana wzmocnieniem gradientowym.\n","Wzmocnienie gradientowe to metoda, która przechodzi przez cykle, aby iteracyjnie dodawać modele do zespołu.\n","\n","Rozpoczyna się inicjalizacją zespołu za pomocą pojedynczego modelu, którego przewidywania mogą być dość naiwne. (Nawet jeśli jego przewidywania są szalenie niedokładne, kolejne dodatki do zespołu rozwiązują te błędy).\n","\n"]},{"cell_type":"code","metadata":{"id":"xRdBnXlkdiEg"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Odczyt danych\n","data = pd.read_csv('/content/melb_data.csv')\n","\n","# Wybór podzbioru predyktorów\n","cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\n","X = data[cols_to_use]\n","\n","# Wybór celu\n","y = data.Price\n","\n","# Oddzielenie danych do zbiorów uczących i walidacyjnych\n","X_train, X_valid, y_train, y_valid = train_test_split(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Y_wqeoGdzbP"},"source":["W tym przykładzie  pracowałem z biblioteką XGBoost. XGBoost oznacza ekstremalne wzmocnienie gradientu, które jest implementacją wzmocnienia gradientu z kilkoma dodatkowymi funkcjami skoncentrowanymi na wydajności i szybkości. (Scikit-learn ma inną wersję wzmocnienia gradientu, ale XGBoost ma pewne zalety techniczne).\n","\n","W następnej komórce kodu importujemy scikit-learn API dla XGBoost (xgboost.XGBRegressor). To pozwala nam budować i dopasowywać model tak, jak robilibyśmy to w scikit-learn. Jak zobaczysz na wyjściu, klasa XGBRegressor ma wiele parametrów do strojenia - dowiesz się o nich wkrótce!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1x3ru3Qd2xv","outputId":"ce6f1ca2-8766-4418-8d10-88a104c7c2cf"},"source":["from xgboost import XGBRegressor\n","\n","my_model = XGBRegressor()\n","my_model.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[18:13:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","             colsample_bynode=1, colsample_bytree=1, gamma=0,\n","             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n","             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n","             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n","             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","             silent=None, subsample=1, verbosity=1)"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2o5lzSwed8f1","outputId":"fac8ed3d-e9c3-4d3d-95e0-eeb4e34375fe"},"source":["from sklearn.metrics import mean_absolute_error\n","\n","predictions = my_model.predict(X_valid)\n","print(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mean Absolute Error: 267505.45321704715\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2x3hSE3ZeCc1"},"source":["Strojenie parametrów\n","\n","XGBoost ma kilka parametrów, które mogą radykalnie wpłynąć na dokładność i szybkość treningu. Pierwsze parametry, które powiniśmy zrozumieć, to:\n","n_estimators\n","\n","n_estimators określa, ile razy należy przejść przez cykl modelowania opisany powyżej. Jest równa liczbie modeli, które włączamy do zespołu.\n","\n","Zbyt niska wartość powoduje niedopasowanie, co prowadzi do niedokładnych prognoz zarówno dotyczących danych uczących, jak i danych testowych.\n","Zbyt duża wartość powoduje nadmierne dopasowanie, co powoduje trafne przewidywania dotyczące danych treningowych, ale niedokładne przewidywania dotyczące danych testowych (na czym nam zależy).\n","\n","Typowe wartości mieszczą się w zakresie 100-1000, chociaż zależy to w dużej mierze od parametru learning_rate omówionego poniżej.\n","\n","Oto kod do ustawiania liczby modeli w zestawie:"]},{"cell_type":"code","metadata":{"id":"kVrrW7zaeKgs"},"source":["my_model = XGBRegressor(n_estimators=500)\n","my_model.fit(X_train, y_train)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tKeh1r59eTJJ"},"source":["early_stopping_rounds\n","\n","early_stopping_rounds oferuje sposób na automatyczne znalezienie idealnej wartości dla n_estimators. Wczesne zatrzymanie powoduje, że model przestaje iterować, gdy wynik walidacji przestaje się poprawiać, nawet jeśli nie jesteśmy na twardym zatrzymaniu dla n_estimators. Rozsądnie jest ustawić wysoką wartość dla n_estimators, a następnie użyć early_stopping_rounds, aby znaleźć optymalny czas na zatrzymanie iteracji.\n","\n","Ponieważ przypadkowa szansa czasami powoduje pojedynczą rundę, w której wyniki walidacji nie ulegają poprawie, przed zatrzymaniem należy określić liczbę, na ile rund pogorszenia jakości prostej można pozwolić. Ustawienie early_stopping_rounds = 5 jest rozsądnym wyborem. W takim przypadku zatrzymujemy się po 5 prostych rundach z pogarszającymi się wynikami walidacji.\n","\n","Korzystając z early_stopping_rounds, musisz również odłożyć na bok niektóre dane do obliczenia wyników walidacji - odbywa się to poprzez ustawienie parametru eval_set.\n","\n","Możemy zmodyfikować powyższy przykład, aby uwzględnić wczesne zatrzymanie:"]},{"cell_type":"code","metadata":{"id":"212iE1KeeXdj"},"source":["my_model = XGBRegressor(n_estimators=500)\n","my_model.fit(X_train, y_train, \n","             early_stopping_rounds=5, \n","             eval_set=[(X_valid, y_valid)],\n","             verbose=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8G31J9K1ebni"},"source":["Jeśli później chcemy dopasować model do wszystkich swoich danych, ustaw n_estimators na dowolną wartość, którą uznasz za optymalną, gdy jest uruchamiany z wczesnym zatrzymaniem.\n","learning_rate\n","\n","Zamiast uzyskiwać prognozy po prostu sumując prognozy z każdego modelu składowego, możemy pomnożyć prognozy z każdego modelu przez niewielką liczbę (znaną jako współczynnik uczenia się) przed ich dodaniem.\n","\n","Oznacza to, że każde drzewo, które dodajemy do zespołu, pomaga nam mniej. Więc możemy ustawić wyższą wartość dla n_estimators bez przeuczenia. Jeśli zastosujemy wczesne zatrzymywanie, odpowiednia liczba drzew zostanie określona automatycznie.\n","\n","Ogólnie rzecz biorąc, mały współczynnik uczenia się i duża liczba estymatorów dadzą dokładniejsze modele XGBoost, chociaż trenowanie modelu zajmie więcej czasu, ponieważ wykonuje on więcej iteracji w całym cyklu. Domyślnie XGBoost ustawia learning_rate = 0,1.\n","\n","Modyfikacja powyższego przykładu w celu zmiany współczynnika uczenia daje następujący kod:"]},{"cell_type":"code","metadata":{"id":"ogA2QoNZefzK"},"source":["my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n","my_model.fit(X_train, y_train, \n","             early_stopping_rounds=5, \n","             eval_set=[(X_valid, y_valid)], \n","             verbose=False)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z7gbEqNVejVj"},"source":["n_jobs\n","\n","W przypadku większych zbiorów danych, w których bierze się pod uwagę czas wykonywania, można użyć równoległości, aby szybciej budować modele. Często ustawia się parametr n_jobs równy liczbie rdzeni na komputerze. W przypadku mniejszych zbiorów danych to nie pomoże.\n","\n","Wynikowy model nie będzie lepszy, więc mikro-optymalizacja pod kątem czasu dopasowania jest zazwyczaj niczym innym jak rozproszeniem. Jest to jednak przydatne w przypadku dużych zbiorów danych, w których w innym przypadku spędziłbym dużo czasu na oczekiwaniu na wykonanie polecenia dopasowania.\n","\n","Oto zmodyfikowany przykład:"]},{"cell_type":"code","metadata":{"id":"OXcArqwpenRs"},"source":["my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n","my_model.fit(X_train, y_train, \n","             early_stopping_rounds=5, \n","             eval_set=[(X_valid, y_valid)], \n","             verbose=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VriRmkvaewq3"},"source":["Wniosek\n","\n","XGBoost to wiodąca biblioteka oprogramowania do pracy ze standardowymi danymi tabelarycznymi (typ danych przechowywanych w Pandas DataFrames, w przeciwieństwie do bardziej egzotycznych typów danych, takich jak obrazy i filmy). Dzięki starannemu dostrajaniu parametrów można trenować bardzo dokładne modele."]},{"cell_type":"markdown","metadata":{"id":"uzAOQRWVe1XY"},"source":["# 7. Wyciek danych\n","Wyciek danych (lub wyciek) ma miejsce, gdy dane szkoleniowe zawierają informacje o celu, ale podobne dane nie będą dostępne, gdy model jest używany do prognozowania. Prowadzi to do wysokiej wydajności zbioru uczącego (i być może nawet danych walidacyjnych), ale model będzie działał słabo w produkcji.\n","\n","Innymi słowy, wyciek powoduje, że model wygląda dokładnie, dopóki nie zaczniesz podejmować decyzji z modelem, a wtedy model staje się bardzo niedokładny.\n","\n","Istnieją dwa główne rodzaje wycieków: wyciek docelowy i zanieczyszczenie podczas testu treningu:\n","-Docelowy wyciek-występuje, gdy predyktory zawierają dane, które nie będą dostępne w momencie tworzenia prognoz.-Zanieczyszczenie testem treningowym-inny typ wycieku występuje, gdy nie jesteśmy ostrożni przy odróżnianiu danych treningowych od danych walidacyjnych."]},{"cell_type":"markdown","metadata":{"id":"5ALp_O2WfbQs"},"source":["Przykład\n","\n","W tym przykładzie pokazany jest sposób wykrywania i usuwania docelowego wycieku.\n","\n","Wykorzystamy zbiór danych o aplikacjach do kart kredytowych i pominiemy podstawowy kod konfiguracji danych. W rezultacie informacje o każdym wniosku o kartę kredytową są przechowywane w DataFrame X. Wykorzystamy je do przewidzenia, które wnioski zostały zaakceptowane w serii Y.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"FnOosOaefjcs","outputId":"e2bababe-45c2-4c74-f724-8d75319d82ea"},"source":["\n","\n","import pandas as pd\n","\n","# Read the data\n","data = pd.read_csv('/content/AER_credit_card_data.csv', \n","                   true_values = ['yes'], false_values = ['no'])\n","\n","# Select target\n","y = data.card\n","\n","# Select predictors\n","X = data.drop(['card'], axis=1)\n","\n","print(\"Liczba wierszy w zbiorze danych:\", X.shape[0])\n","X.head()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Liczba wierszy w zbiorze danych: 1319\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>reports</th>\n","      <th>age</th>\n","      <th>income</th>\n","      <th>share</th>\n","      <th>expenditure</th>\n","      <th>owner</th>\n","      <th>selfemp</th>\n","      <th>dependents</th>\n","      <th>months</th>\n","      <th>majorcards</th>\n","      <th>active</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>37.66667</td>\n","      <td>4.5200</td>\n","      <td>0.033270</td>\n","      <td>124.983300</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>3</td>\n","      <td>54</td>\n","      <td>1</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>33.25000</td>\n","      <td>2.4200</td>\n","      <td>0.005217</td>\n","      <td>9.854167</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>3</td>\n","      <td>34</td>\n","      <td>1</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>33.66667</td>\n","      <td>4.5000</td>\n","      <td>0.004156</td>\n","      <td>15.000000</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>4</td>\n","      <td>58</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>30.50000</td>\n","      <td>2.5400</td>\n","      <td>0.065214</td>\n","      <td>137.869200</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>25</td>\n","      <td>1</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>32.16667</td>\n","      <td>9.7867</td>\n","      <td>0.067051</td>\n","      <td>546.503300</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>2</td>\n","      <td>64</td>\n","      <td>1</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   reports       age  income     share  ...  dependents  months  majorcards  active\n","0        0  37.66667  4.5200  0.033270  ...           3      54           1      12\n","1        0  33.25000  2.4200  0.005217  ...           3      34           1      13\n","2        0  33.66667  4.5000  0.004156  ...           4      58           1       5\n","3        0  30.50000  2.5400  0.065214  ...           0      25           1       7\n","4        0  32.16667  9.7867  0.067051  ...           2      64           1       5\n","\n","[5 rows x 11 columns]"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"X5cl-f2lhQE_"},"source":["Ponieważ jest to mały zbiór danych, użyjemy walidacji krzyżowej, aby zapewnić dokładne pomiary jakości modelu."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zCy0GVuIgo3k","outputId":"03b28922-6cb5-4ebc-80b3-325e3a8cb6f6"},"source":["from sklearn.pipeline import make_pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import cross_val_score\n","\n","# Since there is no preprocessing, we don't need a pipeline (used anyway as best practice!)\n","my_pipeline = make_pipeline(RandomForestClassifier(n_estimators=100))\n","cv_scores = cross_val_score(my_pipeline, X, y, \n","                            cv=5,\n","                            scoring='accuracy')\n","\n","print(\"Cross-validation precyzja: %f\" % cv_scores.mean())\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cross-validation precyzja: 0.981049\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dwGXgY7Sh528"},"source":["Kilka zmiennych wygląda podejrzanie. Na przykład, czy wydatek oznacza wydatek na tę kartę lub na karty użyte przed złożeniem wniosku?\n","\n","W tym momencie podstawowe porównania danych mogą być bardzo pomocne:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a__E1qmOiAVy","outputId":"2f267077-5b55-4a40-8457-d9d1a62293c4"},"source":["expenditures_cardholders = X.expenditure[y]\n","expenditures_noncardholders = X.expenditure[~y]\n","\n","print('Odsetek osób, które nie otrzymały karty i nie poniosły żadnych wydatków: %.2f' \\\n","      %((expenditures_noncardholders == 0).mean()))\n","print('Odsetek tych, którzy otrzymali kartę i nie ponieśli żadnych wydatków: %.2f' \\\n","      %(( expenditures_cardholders == 0).mean()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Odsetek osób, które nie otrzymały karty i nie poniosły żadnych wydatków: 1.00\n","Odsetek tych, którzy otrzymali kartę i nie ponieśli żadnych wydatków: 0.02\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zRiV4ZN3iFW6"},"source":["Jak pokazano powyżej, każdy, kto nie otrzymał karty, nie ponosił żadnych wydatków, podczas gdy tylko 2% osób, które otrzymały kartę, nie miało żadnych wydatków. Nic dziwnego, że nasz model wydawał się mieć dużą dokładność. Ale wydaje się, że jest to również przypadek wycieku docelowego, gdzie wydatki prawdopodobnie oznaczają wydatki na kartę, o którą się ubiegali.\n","\n","Ponieważ udział częściowo zależy od wydatków, należy go również wykluczyć. Zmienne aktywne i karty główne są trochę mniej jasne, ale z opisu brzmią niepokojąco. W większości sytuacji lepiej być bezpiecznym niż żałować, jeśli nie możesz wyśledzić osób, które utworzyły dane, aby dowiedzieć się więcej.\n","\n","Uruchomilibyśmy model bez docelowego wycieku w następujący sposób:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3tucqaiiQgY","outputId":"c586fac1-a656-48c2-c2de-8b09c9fae264"},"source":["# Usuwanie nieszczelnych predyktorów ze zbioru danych\n","potential_leaks = ['expenditure', 'share', 'active', 'majorcards']\n","X2 = X.drop(potential_leaks, axis=1)\n","\n","# Ocena modelu z usuniętymi nieszczelnymi predyktorami\n","cv_scores = cross_val_score(my_pipeline, X2, y, \n","                            cv=5,\n","                            scoring='accuracy')\n","\n","print(\"Cross-val precyzja: %f\" % cv_scores.mean())\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cross-val precyzja: 0.830934\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9m9Oh2GSifJJ"},"source":["Ta dokładność jest nieco niższa, co może być rozczarowujące. Możemy jednak oczekiwać, że w około 80% przypadków będzie to poprawne, gdy będzie używane w nowych aplikacjach, podczas gdy nieszczelny model prawdopodobnie wypadłby znacznie gorzej (pomimo wyższego pozornego wyniku w walidacji krzyżowej).\n","\n","Wyciek danych może być wielomilionowym błędem w wielu aplikacjach do nauki o danych. Staranne rozdzielenie danych treningowych i walidacyjnych może zapobiec zanieczyszczeniu testów pociągu, a rurociągi mogą pomóc we wdrożeniu tego oddzielenia. Podobnie, połączenie ostrożności, zdrowego rozsądku i eksploracji danych może pomóc zidentyfikować wyciek docelowy."]}]}